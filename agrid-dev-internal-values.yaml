# Custom values yaml file containing environment specific values and product specific custom configs
elasticsearch:
  enabled: false
  image: "alphaf2bacr.azurecr.io/alpha-logging/docker.elastic.co/elasticsearch/elasticsearch"
  imageTag: "7.12.1"
  replicas: 6
  minimumMasterNodes: 3
  esJavaOpts: "-Xmx8g -Xms8g"
  resources:
    requests:
      cpu: "4000m"
      memory: "16Gi"
    limits:
      cpu: "4000m"
      memory: "16Gi"

  persistence:  
    labels:
      # Add default labels for the volumeClaimTemplate of the StatefulSet
      enabled: true      
  volumeClaimTemplate:
    accessModes: [ "ReadWriteOnce" ]
    storageClassName: "obs-elastic-sc"
    resources:
      requests:
        storage: 1Ti
  podSecurityPolicy:
    create: true
    name: ""
    spec:
      privileged: true
      fsGroup:
        rule: RunAsAny
      runAsUser:
        rule: RunAsAny
      seLinux:
        rule: RunAsAny
      supplementalGroups:
        rule: RunAsAny
      volumes:
        - secret
        - configMap
        - persistentVolumeClaim
        - emptyDir
  rbac:
    create: true
    serviceAccountAnnotations: {}
    serviceAccountName: ""
  ingress:
    enabled: true
    annotations:
      kubernetes.io/ingress.class: obs-nginx-ingress
      nginx.ingress.kubernetes.io/rewrite-target: /$2
      nginx.ingress.kubernetes.io/auth-url: "https://$host/oauth2/auth"
      nginx.ingress.kubernetes.io/auth-signin: "https://$host/oauth2/start?rd=$escaped_request_uri"
    hosts:
      - host: dr-monitoring-az.crd.com
        paths:
          - path: /elasticsearch(/|$)(.*)
  esConfig:
    elasticsearch.yml: |
      http.max_content_length: 2000mb

  lifecycle:
    preStop:
      exec:
        command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
    postStart:
      exec:
        command:
          - bash
          - -c
          - |
            #!/bin/bash
            # Add a template to adjust number of shards/replicas
            TEMPLATE_NAME=agrid_logstash_template
            INDEX_PATTERN=\"logstash-*\",\"alpha-tracing-*\"
            SHARD_COUNT=3
            REPLICA_COUNT=1
            ES_URL=http://localhost:9200
            while [[ "$(curl -s -o /dev/null -w '%{http_code}\n' $ES_URL)" != "200" ]]; do sleep 1; done
            curl -XPUT "$ES_URL/_ilm/policy/agrid_ilm_policy" -H 'Content-Type: application/json' -d'{"policy": {  "phases": { "hot": {"min_age": "0ms","actions": {} }, "delete": {"min_age": "1d","actions": {  "delete": { "delete_searchable_snapshot": true  }} }  }} }'
            curl -XPUT "$ES_URL/_index_template/$TEMPLATE_NAME" -H 'Content-Type: application/json' -d'{"index_patterns":['"$INDEX_PATTERN"'],"template": {  "settings":{ "number_of_shards": '$SHARD_COUNT', "number_of_replicas": '$REPLICA_COUNT', "index": {  "lifecycle": { "name": "agrid_ilm_policy"  },  "codec": "best_compression",  "query": { "default_field": ["log" ]  } }  }} }'


kibana:
  enabled: false  
  image: "alphaf2bacr.azurecr.io/alpha-logging/docker.elastic.co/kibana/kibana"
  imageTag: "7.12.1"
  replicas: 3
  kibanaConfig:
    kibana.yml: |
      server.basePath: "/kibana"
      server.rewriteBasePath: true
  healthCheckPath: "/kibana/app/kibana"
  ingress:
    enabled: true
    annotations: 
      kubernetes.io/ingress.class: obs-nginx-ingress
    hosts:
      - host: dr-monitoring-az.crd.com
        paths:
          - path: /kibana/

fluentd:
  enabled: false  
  image:
    repository: "alphaf2bacr.azurecr.io/alpha-logging/fluent/fluentd-kubernetes-daemonset"
    tag: "v1.12.3-debian-elasticsearch7-1.0"
  fileConfigs:
    04_outputs.conf: |-
      <label @OUTPUT>
        <match **>
          @type elasticsearch_dynamic
          logstash_format true
          suppress_type_name true
          logstash_prefix logstash-${record['kubernetes']['namespace_name']}
          host "elasticsearch-master"
          port 9200
          path ""
          user elastic
          password changeme

          request_timeout 2147483648
          reload_connections false
          reload_on_failure true
          reconnect_on_error true

          <buffer>
            @type file
            path /var/log/fluentd-buffers/kubernetes.system.buffer
            chunk_limit_size 128MB
            overflow_action block
            queued_chunks_limit_size 8
            retry_max_interval 30
            retry_type periodic
            retry_wait 60s
            flush_thread_count 8
            flush_interval 20s
            retry_forever false          
          </buffer> 
        
        </match>
      </label>
  metrics:
    serviceMonitor:
      enabled: true
      additionalLabels:
        release: prometheus-operator
      namespace: ""
      namespaceSelector: {}
      ## metric relabel configs to apply to samples before ingestion.
      ##
      metricRelabelings: []
      # - sourceLabels: [__name__]
      #   separator: ;
      #   regex: ^fluentd_output_status_buffer_(oldest|newest)_.+
      #   replacement: $1
      #   action: drop
      ## relabel configs to apply to samples after ingestion.
      ##
      relabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace
      ## Additional serviceMonitor config
      ##
      jobLabel: fluentd
      scrapeInterval: 30s
      scrapeTimeout: 5s
      honorLabels: true

    prometheusRule:
      enabled: true
      additionalLabels: {}
      namespace: ""
      rules:
      - alert: FluentdDown
        expr: up{job="fluentd"} == 0
        for: 5m
        labels:
          context: fluentd
          severity: warning
        annotations:
          summary: "Fluentd Down"
          description: "{{ $labels.pod }} on {{ $labels.nodename }} is down"
      - alert: FluentdScrapeMissing
        expr: absent(up{job="fluentd"} == 1)
        for: 15m
        labels:
          context: fluentd
          severity: warning
        annotations:
          summary: "Fluentd Scrape Missing"
          description: "Fluentd instance has disappeared from Prometheus target discovery"

fluent-bit:
  enabled: false
